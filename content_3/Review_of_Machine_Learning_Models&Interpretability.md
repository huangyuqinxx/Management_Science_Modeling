# 机器学习模型可解释性研究现状

&emsp; 为了在机器学习(ML)系统中达到较高的精度，实践者通常使用复杂的“黑箱”模型，这些模型不容易被人类理解。这种模型的不透明导致了公众对在高风险环境中使用它们的担忧，如果这些预测背后的原因是一个高度非线性的谜，不容易破译，这些预测可信性将受到质疑。

&emsp; 可解释人工智能(ExAI)领域吸引了研究人员开发黑盒深度网络的可解释方法（Tim Miller，2019）<sup>[29]</sup>。虽然其中一些方法可以直接应用于NLP模型[6,63,88]，但其他方法仅适用于图像数据集或卷积神经网络(CNNs)，这些方法并不非常适合于神经机器理解任务（Sebastian Bach等，2015）<sup>[30]</sup>。Holzinger A等（2019）<sup>[25]</sup>提供了一些必要的定义来区分可解释性和因果性，以及组织病理学中DL解释和人类解释的实例。MILLER JANNY等（2020）<sup>[27] </sup>评估了著名的逻辑回归模型和几个机器学习算法在P2P借贷中授予评分。比较结果表明，机器学习方法不仅在分类性能上有优势，而且在可解释性上也有优势。

&emsp; Shafie Gholizadeh等（2021）<sup>[31]</sup> 将一种NLP可解释性方法-层相关传播(LRP)应用于NLP分类模型。我们使用LRP方法为实例中的每个单词推导一个相关性得分，这是一个局部可解释性。然后将相关性分数聚合在一起，以获得模型的全局变量重要性。Andrew Bell等（2022）<sup>[26]</sup>在两个现实世界的政策环境中，对模型准确性和可解释性之间权衡的经验量化，将可解释性量化为一个在循环中的人(HITL)对模型的理解程度，使用了客观可测量标准的组合。JULIA EL ZINI等（2022）<sup>[28]</sup> 的调查集中在语言模型或操作文本数据的深度网络的可解释性上，提出了一项研究NLP模型的模型不可知论和模型特定解释方法的调查。这些方法既可以开发固有可解释的NLP模型，也可以以事后的方式对预先训练的模型进行操作。
